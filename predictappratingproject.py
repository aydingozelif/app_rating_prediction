# -*- coding: utf-8 -*-
"""PredictAppRatingProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ung-0ceO9FtUvmrz9QRTgWY7lF3VdFQU
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn import metrics
import random
import matplotlib.pyplot as plt
# %matplotlib inline
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop,Nadam,Adadelta,Adam
from tensorflow.keras.layers import BatchNormalization,LeakyReLU
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

#Data transferred from csv file to variable named df
df = pd.read_csv("./googleplaystore.csv")

"""# Data Preprocessing"""

#Details of the data frame were examined.
df.info()

#It was observed that there were empty entries in some columns.

#Rows with blank cells were removed from the data frame
df.dropna(inplace = True)

df.info()
#Now I have 9360 rows and each column has a value

"""For the steps below, to process data in machine learning algorithms, we first need to convert from text to numbers, because most algorithms work better that way. Data cleanup / preprocessing is the most important part of any machine learning process as high quality data transforms into high quality predictions and models."""

#Details of the data frame columns were examined.
df.head()

"""From the categorical column, I converted each category into an individual number. In the later sections when we do apply machine learning, two methods will be applied to the code, being integer encoding(which we are doing now) and one-hot encoding, aka dummy variables.

The main reason as to why I understand we do this transformation is mainly because integer encoding relies on the fact that there's a relationship between each category(e.g. think age range vs types of animals). In this case however, it's hard to really determine such a relationship, hence dummy/one-hot encoding might help provide better predictive accuracy.
"""

# Cleaning categories into integers
CategoryString = df["Category"]
categoryVal = df["Category"].unique()
categoryValCount = len(categoryVal)
category_dict = {}
for i in range(0,categoryValCount):
    category_dict[categoryVal[i]] = i
df["Category_c"] = df["Category"].map(category_dict).astype(int)

"""Cleaning of sizes of the apps and also filling up the missing values using ffill"""

#scaling and cleaning size of installation
def change_size(size):
    if 'M' in size:
        x = size[:-1]
        x = float(x)*1000000
        return(x)
    elif 'k' == size[-1:]:
        x = size[:-1]
        x = float(x)*1000
        return(x)
    else:
        return None

df["Size"] = df["Size"].map(change_size)

#filling Size which had NA
df.Size.fillna(method = 'ffill', inplace = True)

df.info()

"""Cleaning the number of installations column

"""

#Cleaning no of installs classification
df['Installs'] = [int(i[:-1].replace(',','')) for i in df['Installs']]



df.head()
#Now I have 9360 rows and each column has a value

"""Converting the paid/free classification types into binary"""

#Converting Type classification into binary
def type_cat(types):
    if types == 'Free':
        return 0
    else:
        return 1

df['Type'] = df['Type'].map(type_cat)

"""Converting of the content rating section into integers. In this specific instance, given that the content rating is somewhat relatable and has an order to it, we do not use one-hot encoding."""

#Cleaning of content rating classification
RatingL = df['Content Rating'].unique()
RatingDict = {}
for i in range(len(RatingL)):
    RatingDict[RatingL[i]] = i
df['Content Rating'] = df['Content Rating'].map(RatingDict).astype(int)

"""I dropped these portions of information as i deemed it unecessary for our machine learning algorithm"""

#dropping of unrelated and unnecessary items
df.drop(labels = ['Last Updated','Current Ver','Android Ver','App'], axis = 1, inplace = True)

"""Technically when doing the cleaning of genres, one-hot should also be applied in this instance. However, I did not as firstly, it's a subset of the categorical column and secondly, application of a dummy variable would significantly increase the number of independent variables.

So to combat this instead, we ran two seperate regressions, one including and one excluding such genre data. When including the data, we only considered in the impact/information provided via the genre section purely based on it's numeric value.
"""

#Cleaning of genres
GenresL = df.Genres.unique()
GenresDict = {}
for i in range(len(GenresL)):
    GenresDict[GenresL[i]] = i
df['Genres_c'] = df['Genres'].map(GenresDict).astype(int)

"""Cleaning of the prices of the apps to floats"""

def price_clean(price):
    if price == '0':
        return 0
    else:
        price = price[1:]
        price = float(price)
        return price

df['Price'] = df['Price'].map(price_clean).astype(float)

"""Converting the number reviews column into integers"""

# convert reviews to numeric
df['Reviews'] = df['Reviews'].astype(int)

df.info()

"""Converted all columns except my target field to the format I should use."""

#round float value to nearest int value
def proper_round(num, dec=0):
    num = str(num)[:str(num).index('.')+dec+2]
    if num[-1]>='5':
        return int(float(num[:-2-(not dec)]+str(int(num[-2-(not dec)])+1)))
    return int(float(num[:-1]))

df["Rating"] = df["Rating"].map(proper_round)

# convert int to
df["Rating"] = df["Rating"].astype('category')

df.head()

"""I finished the column based data cleaning. Now I'm starting row-based data cleaning"""

print("There is {} duplicated values in data frame".format(df.duplicated().sum()))

"""I can leave duplicate data as it has no effect on model training.
First of all, I want to see the repeating lines visually.
"""

duplicated = df[df.duplicated(keep=False)]
duplicated = duplicated.sort_values(by=["Rating", "Reviews", "Size", "Installs"], ascending= False)

duplicated.head(10)

df.drop_duplicates(inplace=True)
print("There is {} duplicated values in data frame".format(df.duplicated().sum()))

"""Now my dataset is clean.

# Training & Testing of Model
"""

X = df.drop(labels = ['Category','Rating','Genres','Genres_c'],axis = 1)
y = df.Rating

from sklearn.model_selection import train_test_split
x_train,x_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)

from sklearn.preprocessing import normalize
x_train = normalize(x_train)
x_test = normalize(x_test)
X = normalize(X)



"""# Model Comparison"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

dec = DecisionTreeClassifier()
ran = RandomForestClassifier(n_estimators=100)
knn = KNeighborsClassifier(n_neighbors=100)

models = {"Decision tree" : dec,
          "Random forest" : ran,
          "KNN" : knn}
scores= { }

for key, value in models.items():
    model = value
    model.fit(x_train, y_train)
    scores[key] = model.score(x_test, y_test)

scores_frame = pd.DataFrame(scores, index=["Accuracy Score"]).T
scores_frame.sort_values(by=["Accuracy Score"], axis=0 ,ascending=False, inplace=True)
scores_frame

plt.figure(figsize=(5,5))
sns.barplot(x=scores_frame.index,y=scores_frame["Accuracy Score"])
plt.xticks(rotation=45)

"""It seems that the KNN and Random Forest algorithms are way ahead of the others. So let's focus on these algorithms

## K Fold Cross Validation
With K Fold Cross Validation, we get results from "K" mini training sets, different from our main training set.
We then choose the average of these results as the actual result.
Ultimately, we can examine whether the data are consistent or not by taking the standard deviation of the K result.
"""

from sklearn.model_selection import cross_val_score
accuracies_random_forest = cross_val_score(estimator=ran, X=x_train, y=y_train, cv=10)
accuracies_knn = cross_val_score(estimator=knn, X=x_train, y=y_train, cv=10)

print("Random Forest Average accuracy: ", accuracies_random_forest.mean())
print("Random Forest Standart Deviation: ", accuracies_random_forest.std())
print("KNN Average accuracy: ", accuracies_knn.mean())
print("KNN Standart Deviation: ", accuracies_knn.std())

"""The standard deviation value indicates that we are getting consistent results. In other words, the smaller our standard deviation value, the more consistent the results we obtain.

### Grid Search <a id="13"></a>

<hr>
Let's use the grid search algorithm to find the best Random Forest Algorithms "n_estimators" hyperparameter value for our model.
"""

from sklearn.model_selection import GridSearchCV

grid = {"n_estimators" : np.arange(10,150,10)}

ran_cv = GridSearchCV(ran, grid, cv=3) # GridSearchCV
ran_cv.fit(x_train,y_train)# Fit

print("Tuned hyperparameter n_estimators: {}".format(ran_cv.best_params_))
print("Best score: {}".format(ran_cv.best_score_))

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(solver="liblinear", max_iter=200)
grid = {"penalty" : ["l1", "l2"],
         "C" : np.arange(60,80,2)} # (60,62,64 ... 78)
log_reg_cv = GridSearchCV(log_reg, grid, cv=3)
log_reg_cv.fit(x_train, y_train)


print("Tuned hyperparameter n_estimators: {}".format(log_reg_cv.best_params_))
print("Best score: {}".format(log_reg_cv.best_score_))

"""# Model Evaluation

### Test Set Accuracy Score <a id="15"> </a>

We have now selected our model with better hyper parameters than the default. We can evaluate the model with our test set.
"""

logreg_best = LogisticRegression(C=74, penalty="l1", solver="liblinear")
logreg_best.fit(x_train, y_train)
print("Test accuracy: ",logreg_best.score(x_test, y_test))

"""The ultimate success of our model in predicting the test set not used in education is 0.61.

### Confusion Matrix <a id="16"></a>
"""

y_true = y_test
y_pred = logreg_best.predict(x_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_pred)

f, ax = plt.subplots(figsize=(5,5))
sns.heatmap(cm,fmt=".0f", annot=True,linewidths=0.2, linecolor="purple", ax=ax)
plt.xlabel("Predicted")
plt.ylabel("Grand Truth")
plt.show()

"""### F1 Score <a id="17"></a>

<hr>
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_true, y_pred)))

print('Micro Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_true, y_pred, average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_true, y_pred, average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_true, y_pred, average='weighted')))

from sklearn.metrics import classification_report
print('\nClassification Report\n')
print(classification_report(y_true, y_pred, target_names=["1", "2", "3", "4", "5"]))

"""The harmonic mean of precision and recall is F1-Score. If we are looking for a balance between precision and recall, we look for situations where F1-Score is maximum. """